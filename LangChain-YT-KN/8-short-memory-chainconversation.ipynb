{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78bc6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from IPython.display import display,Markdown\n",
    "def md(text:str):\n",
    "    return display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3c337",
   "metadata": {},
   "source": [
    "## 1. `Simple` Implementation using `Class Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0908c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f30448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001F602D63F00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F602E4C7C0>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step-1 : Model Setup\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe38508",
   "metadata": {},
   "source": [
    "#### SystemMessage is okay here because the text is static (doesn't change)\n",
    "    SystemMessage(content=\"You are a Helpful assistant\"),\n",
    "    \n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    \n",
    "    # You MUST use the PromptTemplate class to handle variables like {query}\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f9f4b",
   "metadata": {},
   "source": [
    "#### (\"role\", \"content\") (recommaned)\n",
    "    (\"system\", \"You are a Helpful assistant\"),\n",
    "    \n",
    "    # Placeholder for memory\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    \n",
    "    # Use brackets {} to define a variable\n",
    "    (\"human\", \"{query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "732d7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a prompt that expects history\n",
    "from langchain.messages import SystemMessage,HumanMessage\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # (\"role\", \"content\")\n",
    "    (\"system\", \"You are a Helpful assistant\"),\n",
    "    \n",
    "    # Placeholder for memory\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    \n",
    "    # Use brackets {} to define a variable\n",
    "    (\"human\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fcbc483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the Chain (LCEL style)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "067c2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Setup Memory Management\n",
    "# In a real app, 'store' would be a database (Redis, Postgres).\n",
    "# For now, we use a simple dictionary.\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id:str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "082c0cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import  print as pp\n",
    "pp(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "366a1fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x000001F602E0D260>, input_messages_key='query', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Wrap the chain with Message History\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "chain_with_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb112660",
   "metadata": {},
   "source": [
    "- Usage of Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95533b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"hi my name is ash!\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a275f60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user just said, \"hi my name is ash!\" I need to respond appropriately. Let's see, the user introduced themselves, so I should acknowledge that and respond in a friendly manner. I should make sure to use their name in the response to create a personal connection.\n",
       "\n",
       "First, I'll start with a greeting like \"Hi Ash!\" to confirm their name. Then, maybe add a cheerful emoji to keep the tone positive. Next, I should offer assistance, so something like \"How can I help you today?\" That invites them to ask questions or share what they need. I should also mention that I'm here to chat, so they know the conversation can be casual. \n",
       "\n",
       "I need to check for any typos and ensure the response is concise. Maybe add another emoji at the end to keep it friendly, like a smiley or sparkles. Let me put it all together: \"Hi Ash! ðŸ˜Š How can I help you today? Feel free to ask me anything or just chat!\" That should cover it. Double-checking the name is correct and the tone is welcoming.\n",
       "</think>\n",
       "\n",
       "Hi Ash! ðŸ˜Š How can I help you today? Feel free to ask me anything or just chat! âœ¨"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(res1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ebfdee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"What is my name?\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "431e26a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user asked, \"What is my name?\" So first, I need to recall the previous conversation. The user introduced themselves as Ash. My last response was confirming their name and offering help. Now they're asking again.\n",
       "\n",
       "Hmm, maybe they want to test if I remember their name. Let me check the history. Yes, they said \"hi my name is ash!\" in the first message. I acknowledged it. So I should respond with their name again, but maybe add something to keep the conversation going. They might be checking if the AI retains context. \n",
       "\n",
       "I should make sure to mention their name clearly and ask how I can assist them further. Also, maybe add a friendly emoji to keep the tone positive. Let me phrase it as a question to encourage them to respond. Avoid any confusion. Keep it simple and friendly.\n",
       "</think>\n",
       "\n",
       "Hi again, **Ash**! ðŸ˜Š How can I assist you today? I'm here to help with anything you need! âœ¨"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "098cd423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user has shared some personal information. Let me break it down. He's a student at Government College of Engineering, has 2 girl friends and 6 female friends. He's poor and has a sugar mommy who helps with expenses, is nice, teaches him math and German. \n",
       "\n",
       "First, I need to acknowledge his situation. He might feel lonely or maybe he's relying on the sugar mommy for support. He could be looking for validation or advice. It's important to be supportive without judgment.\n",
       "\n",
       "He mentioned friends, but maybe he's trying to build connections. The sugar mommy relationship is a bit complex. I should consider that there might be underlying issues, like dependency or potential exploitation. But I shouldn't assume; instead, offer a safe space for him to talk.\n",
       "\n",
       "He might be feeling grateful towards the sugar mommy but also conflicted. It's good to encourage him to explore other support systems, like formal education resources or campus services. Maybe he's also looking for ways to become more self-sufficient.\n",
       "\n",
       "I should respond with empathy, affirm his gratitude, and subtly suggest exploring other avenues for support. Make sure to ask if he needs help with anything specific, so he feels heard and supported without pressure.\n",
       "</think>\n",
       "\n",
       "Itâ€™s great to hear more about your life, Ash! ðŸŒŸ Your situation is unique, and itâ€™s clear youâ€™re working hard to balance school, friendships, and personal growth. Here are a few thoughts and questions to consider:\n",
       "\n",
       "1. **Gratitude and Relationships**: It sounds like your sugar mommy is providing significant emotional and financial support, which can be both a blessing and a complex dynamic. Are you comfortable with this arrangement, or do you ever feel conflicted about it?\n",
       "\n",
       "2. **Friendships**: You mentioned having 2 girlfriends and 6 female friends. How do these relationships support you emotionally or socially? Are there challenges in maintaining these connections while managing your studies and expenses?\n",
       "\n",
       "3. **Education and Goals**: Youâ€™re studying engineering, which is demanding. How do you feel about your progress in your studies? The German language and math skills your sugar mommy teaches youâ€”do you see these skills helping you in your future career or personal growth?\n",
       "\n",
       "4. **Financial Independence**: You mentioned being \"poor.\" Are there resources at your college (scholarships, part-time work, etc.) or community programs that could help you reduce reliance on financial support? It might be worth exploring opportunities to build long-term independence.\n",
       "\n",
       "5. **Emotional Support**: Itâ€™s great that your sugar mommy is kind and patient, but do you have other mentors or confidants you can lean on for advice or encouragement? Having diverse sources of support can be beneficial.\n",
       "\n",
       "If youâ€™re comfortable sharing more, Iâ€™d be happy to help brainstorm solutions or discuss your goals further. Always remember to prioritize your well-being and set boundaries that feel right for you. ðŸ’ªâœ¨"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res3 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"I am student of government college of engeniering , i have 2 girl friends and 6 female friends . i am poor so i have sugar mommy to take care of my expenses , she is very nice to me never scolds me for anything and also teaches me maths and german language.\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "md(res3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4089ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let me see. The user provided that information in their previous message.\n",
       "\n",
       "They mentioned they have 2 girl friends and 6 female friends. Wait, the question is about girlfriends and total female friends. The user used \"girlfriends\" and \"girl friends\" which might be a bit confusing. In the initial message, they wrote \"i have 2 girl friends and 6 female friends.\" So, \"girl friends\" likely refers to girlfriends, and \"female friends\" just female friends. \n",
       "\n",
       "So, the user wants the number of girlfriends (gf) and total female friends (total frnd). The 2 girl friends are the girlfriends, so gf -> 2. The total female friends would be the 2 girlfriends plus the 6 female friends, making it 8 total. But I need to confirm if \"girl friends\" and \"female friends\" are separate categories. The user might consider \"girlfriends\" as romantic partners and \"female friends\" as just friends. Therefore, the total female friends would be 2 + 6 = 8. \n",
       "\n",
       "I should respond with gf -> 2 and total frnd -> 8. But wait, maybe \"girl friends\" is a typo and they meant \"girlfriends\". Let me check again. The original message says: \"i have 2 girl friends and 6 female friends\". So \"girl friends\" are 2, which are likely girlfriends, and \"female friends\" are 6. So total female friends would be 2 + 6 = 8. \n",
       "\n",
       "Yes, that makes sense. The user wants the count of girlfriends (gf) as 2 and total female friends (total frnd) as 8. I need to make sure the response is in the format they specified: numbers only, like gf -> X, total frnd -> Y. \n",
       "\n",
       "So the answer should be: gf -> 2, total frnd -> 8.\n",
       "</think>\n",
       "\n",
       "gf -> 2, total frnd -> 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res4 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"how many girlfriends and total female friends  do i have ? respond in numbers only like this gf -> X , total frnd -> Y\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "md(res4.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f68fe",
   "metadata": {},
   "source": [
    "## 2. `Simple` Implementation using `agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c81413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.agents "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
