{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78bc6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from IPython.display import display,Markdown\n",
    "def md(text:str):\n",
    "    return display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3c337",
   "metadata": {},
   "source": [
    "## 1. `Simple` Implementation using `Class Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0908c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f30448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000028EE297C980>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000028EE297D6A0>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step-1 : Model Setup\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe38508",
   "metadata": {},
   "source": [
    "#### SystemMessage is okay here because the text is static (doesn't change)\n",
    "    SystemMessage(content=\"You are a Helpful assistant\"),\n",
    "    \n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    \n",
    "    # You MUST use the PromptTemplate class to handle variables like {query}\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f9f4b",
   "metadata": {},
   "source": [
    "#### (\"role\", \"content\") (recommaned)\n",
    "    (\"system\", \"You are a Helpful assistant\"),\n",
    "    \n",
    "    # Placeholder for memory\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    \n",
    "    # Use brackets {} to define a variable\n",
    "    (\"human\", \"{query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732d7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a prompt that expects history\n",
    "from langchain.messages import SystemMessage,HumanMessage\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # (\"role\", \"content\")\n",
    "    (\"system\", \"You are a Helpful assistant\"),\n",
    "    \n",
    "    # Placeholder for memory\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    \n",
    "    # Use brackets {} to define a variable\n",
    "    (\"human\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcbc483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the Chain (LCEL style)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067c2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Setup Memory Management\n",
    "# In a real app, 'store' would be a database (Redis, Postgres).\n",
    "# For now, we use a simple dictionary.\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id:str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082c0cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import  print as pp\n",
    "pp(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366a1fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x0000028EE3A1B740>, input_messages_key='query', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Wrap the chain with Message History\n",
    "chain_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "chain_with_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb112660",
   "metadata": {},
   "source": [
    "- Usage of Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95533b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"hi my name is ash!\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a275f60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user introduced their name as Ash. I should acknowledge their greeting and respond in a friendly manner. Let me make sure to address them by name and offer assistance. I need to check for any additional details they might have mentioned, but since they just said \"hi\" and their name, a simple response should suffice. Let me ensure the tone is welcoming and open-ended to encourage them to ask questions or share more if they'd like.\n",
       "</think>\n",
       "\n",
       "Hi Ash! Nice to meet you! How can I assist you today? üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(res1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebfdee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"What is my name?\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431e26a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, the user just asked, \"What is my name?\" Let me check the conversation history.\n",
       "\n",
       "In the previous messages, the user introduced themselves as \"Ash\" and I acknowledged that. Now they're asking again. Maybe they want to confirm if I remember their name correctly. I should make sure to restate their name to show I'm paying attention. It's possible they're testing if I retain information from the conversation. I should respond in a friendly and helpful manner, reaffirmating their name and offering further assistance. No need to overcomplicate it. Just a simple, polite reply should do.\n",
       "</think>\n",
       "\n",
       "Your name is Ash! üòä I remember from our previous message. How can I assist you today, Ash?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(res2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098cd423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user introduced themselves as Ash, a student at a government engineering college. They mentioned having two girlfriends and six female friends, which might be a bit confusing because having two girlfriends implies they're in two romantic relationships, but having six female friends as well. Maybe it's a typo or misunderstanding, but I should note that.\n",
       "\n",
       "Ash is poor and relies on a sugar mommy for expenses. The sugar mommy is described as very supportive, never scolding and helping with math and German. I need to be careful here. Sugar relationships can have complex dynamics, especially when there's a power imbalance. It's important to approach this with sensitivity without making assumptions about the nature of the relationship.\n",
       "\n",
       "First, I should acknowledge the information provided. Then, consider possible areas where Ash might need support. Since they're a student, academic challenges could be an issue. Also, the financial situation might cause stress, and relying on someone else for support could have emotional implications. It's good that the sugar mommy is supportive, but it's worth gently exploring whether Ash feels comfortable in this arrangement or if there are any concerns.\n",
       "\n",
       "I should avoid making judgments about the relationships. Instead, focus on offering help with studies (since the sugar mommy teaches math and German) or general advice on managing finances and relationships. Maybe ask if they need help with anything specific, like study tips, time management, or how to handle their finances in a way that empowers them long-term.\n",
       "\n",
       "Also, considering the cultural context (Government College of Engineering), there might be local resources or student support services they could access. I should mention that as a possibility without assuming they're unaware of it.\n",
       "\n",
       "Need to ensure the response is positive, encouraging, and non-judgmental. Encourage Ash to reach out if they need help with anything, whether it's academic, personal, or related to their financial situation. Emphasize that they're not alone and that there are ways to build a stable future beyond current arrangements.\n",
       "</think>\n",
       "\n",
       "Thank you for sharing about yourself, Ash! It sounds like you're navigating a complex but interesting chapter of life. Here‚Äôs a breakdown of how I can support you:\n",
       "\n",
       "1. **Academic Support**:  \n",
       "   Since you‚Äôre studying engineering and learning German, I‚Äôd be happy to help with **study tips**, **math problems**, or **German language practice**. Just let me know what you‚Äôre working on!\n",
       "\n",
       "2. **Financial & Independence Goals**:  \n",
       "   Relying on someone else for financial support can come with challenges. If you‚Äôre open to it, I can share **budgeting strategies**, **ways to build savings**, or ideas for **part-time work/student jobs** (like tutoring, freelancing, or campus jobs) to help you gain financial independence in the future.\n",
       "\n",
       "3. **Relationship Dynamics**:  \n",
       "   It‚Äôs great that your sugar mommy is supportive, but it‚Äôs also important to reflect on whether this arrangement aligns with your long-term goals. If you ever feel uncertain, I can help brainstorm **ways to set healthy boundaries** or explore relationships that empower you emotionally and financially.\n",
       "\n",
       "4. **Social Life & Friendships**:  \n",
       "   Balancing 2 girlfriends and 6 female friends sounds like a lot! If you‚Äôre ever curious about navigating relationships or social dynamics, I‚Äôm here to listen and offer perspective (no judgment, just honesty).\n",
       "\n",
       "5. **Emotional Wellbeing**:  \n",
       "   If you ever feel overwhelmed by your situations, I can help you process your feelings or suggest **stress-relief techniques** or **resources for mental health support**.\n",
       "\n",
       "**How can I assist you today?** üòä  \n",
       "- Need help with homework/study advice?  \n",
       "- Want to brainstorm financial independence plans?  \n",
       "- Looking for advice on relationships or friendships?  \n",
       "- Just want to talk?  \n",
       "\n",
       "You‚Äôre not alone in figuring this out, and I‚Äôm here to support you in a way that respects your goals and values. Let me know what‚Äôs on your mind! üåü"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res3 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"I am student of government college of engeniering , i have 2 girlfriends and 6 female friends . i am poor so i have sugar mommy to take care of my expenses , she is very nice to me never scolds me for anything and also teaches me maths and german language.\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "md(res3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4089ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's see. The user mentioned they have 2 girl friends and 6 female friends. Wait, the terms \"girlfriends\" and \"female friends\" might be a bit confusing here. The user is asking how many girlfriends and total female friends they have.\n",
       "\n",
       "First, I need to clarify the terms. The user said \"2 girl friends\" and \"6 female friends.\" But the term \"girlfriends\" usually refers to romantic partners, while \"female friends\" are just friends. However, in some contexts, people might use \"girlfriends\" to mean friends in general, but that's less common.\n",
       "\n",
       "Wait, the user wrote \"I have 2 girl friends and 6 female friends.\" So adding those together, 2 + 6 = 8. But the user is asking for the number of girlfriends (gf) and total female friends (total frnd). If \"girlfriends\" are the romantic partners here, then the 2 girl friends might be the romantic partners, and the 6 are just friends. But the user used the term \"girl friends,\" which could be ambiguous. However, in the previous response, the assistant assumed that \"girlfriends\" are the romantic ones and \"female friends\" are friends. So in that case, the answer would be gf -> 2, total frnd -> 8.\n",
       "\n",
       "But wait, the user wrote \"I have 2 girl friends and 6 female friends.\" So if \"girlfriends\" are the romantic ones, then the 2 \"girl friends\" are the romantic partners, and the 6 are friends. Therefore, total female friends would be 2 (girlfriends) + 6 (friends) = 8. But the user is asking for \"gf -> X , total frnd -> Y\". So X is 2 (girlfriends) and Y is 8 (total female friends including the girlfriends).\n",
       "\n",
       "But I need to make sure. Let me check the original message again. The user said \"I have 2 girl friends and 6 female friends.\" So the 2 are \"girl friends,\" which could be ambiguous. However, the assistant in the previous response interpreted \"girlfriends\" as romantic partners. So if the user meant 2 girlfriends and 6 female friends (non-romantic), then total female friends would be 2 + 6 = 8. Therefore, the answer should be gf -> 2, total frnd -> 8.\n",
       "\n",
       "But maybe the user used \"girl friends\" to mean non-romantic friends. Then \"girlfriends\" in the question would refer to romantic partners, which the user hasn't mentioned. Wait, the user actually mentioned \"I have 2 girl friends and 6 female friends,\" but didn't specify any romantic relationships. The assistant's previous response assumed that the 2 girl friends are girlfriends. However, the user might not have any girlfriends and just 2 girl friends and 6 female friends. So if the user is asking how many girlfriends and total female friends they have, but in their message they didn't mention any girlfriends, only friends, then the answer would be gf -> 0, total frnd -> 8. But that contradicts the assistant's previous assumption.\n",
       "\n",
       "Wait, the user's initial message says \"I have 2 girl friends and 6 female friends.\" The assistant's response mentioned \"2 girlfriends\" and \"6 female friends,\" adding up to 8. So the assistant interpreted \"girl friends\" as \"girlfriends.\" Therefore, the answer is gf -> 2, total frnd -> 8. But the user might not have any girlfriends. The confusion here is between \"girl friends\" (friends who are girls) and \"girlfriends\" (romantic partners). The user's message is a bit ambiguous. However, based on the previous assistant's interpretation, the answer would be.gf -> 2, total frnd -> 8. But the user might have intended \"girl friends\" as friends, not romantic partners. So in that case, the answer would be gf -> 0, total frnd -> 8. But the assistant's previous response assumed the 2 were girlfriends. Since the user is asking for numbers based on their own statement, the answer should reflect exactly what they said. If they said \"2 girl friends and 6 female friends,\" and they are asking for \"girlfriends\" and \"total female friends,\" then \"girlfriends\" would be 0 (if they are not romantic) and total female friends would be 2 + 6 = 8. But the assistant's previous message interpreted \"girl friends\" as \"girlfriends.\" So there's a discrepancy here. To resolve this, the correct approach is to take the user's exact wording: \"2 girl friends and 6 female friends.\" If \"girlfriends\" are separate from friends, then the user hasn't mentioned any girlfriends. Therefore, the answer would be gf -> 0, total frnd -> 8. However, the assistant previously assumed the 2 girl friends were girlfriends, but the user might not have intended that. So the accurate answer, based on the user's exact words, is gf -> 0, total frnd -> 8. But this could be a misunderstanding. Alternatively, if the user meant that the 2 are girlfriends and 6 are friends, then it's 2 + 6 = 8. But the user's wording is ambiguous. Since the user is asking for the numbers based on their own message, it's safer to take their words literally unless they clarify. So the answer should be gf -> 0, total frnd -> 8. However, the assistant's previous response might have interpreted \"girl friends\" as \"girlfriends,\" leading to gf -> 2, total frnd -> 8. The user is now asking for clarification, so the assistant should respond based on the user's exact wording unless there's a clear indication otherwise. Therefore, the correct answer is gf -> 0, total frnd -> 8. But the assistant's previous message assumed 2 girlfriends, which might have been a mistake. To avoid confusion, the assistant should clarify. However, the user is now asking for the numbers based on their own message, so the assistant must respond accurately. Therefore, the correct numbers are gf -> 0, total frnd -> 8.\n",
       "</think>\n",
       "\n",
       "gf -> 0 , total frnd -> 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res4 = chain_with_memory.invoke(\n",
    "    {\n",
    "        \"query\":\"how many girlfriends and total female friends  do i have ? respond in numbers only like this gf -> X , total frnd -> Y\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"session_id\":\"user_1\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "md(res4.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f68fe",
   "metadata": {},
   "source": [
    "## 2. `Simple` Implementation using `agent` (Requires LangGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3be8c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.tools import tool\n",
    "from langchain.messages import SystemMessage\n",
    "\n",
    "# --- NEW IMPORTS FOR LANGGRAPH ---\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d94b98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP THE MODEL\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7116361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DEFINE A TOOL (Same as before)\n",
    "@tool\n",
    "def get_weather(location:str):\n",
    "    \"\"\"Returns the weather for a given location\"\"\"\n",
    "    return f\"its rainy in {location}.\"\n",
    "\n",
    "tools_list = [get_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01fb318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MEMORY MANAGEMENT (The LangGraph Way)\n",
    "# Instead of \"RunnableWithMessageHistory\", we use a \"Checkpointer\"\n",
    "# This saves the state of the conversation automatically.\n",
    "temp_memory = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ba84396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_24832\\451996678.py:6: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "# 4. CONSTRUCT THE AGENT\n",
    "# In LangGraph, \"create_react_agent\" replaces \"create_tool_calling_agent\" + \"AgentExecutor\"\n",
    "# \"state_modifier\" is where your System Prompt goes now.\n",
    "system_prompt = \"You are a helpful assistant with tools.\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools_list,\n",
    "    checkpointer=temp_memory,\n",
    "    # messages_modifier=SystemMessage(\"You are very Helpful assistant.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73e1be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph Version: 1.0.6\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "try:\n",
    "    print(\"LangGraph Version:\", importlib.metadata.version(\"langgraph\"))\n",
    "except importlib.metadata.PackageNotFoundError:\n",
    "    print(\"LangGraph is not installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfdecbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"session_123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "200baad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = agent.invoke(\n",
    "    {\"messages\": [(\"human\", \"Hi, my name is Sebastian. The secret word is 'Apple'.\")]}, \n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de02da3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hi, my name is Sebastian. The secret word is 'Apple'.\", additional_kwargs={}, response_metadata={}, id='3b888653-b781-46b2-a01c-143f433854dd'),\n",
       "  AIMessage(content='Hello Sebastian! Nice to meet you. I‚Äôve noted your secret word. How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says hi, introduces themselves, and gives a secret word \"Apple\". Likely they are establishing a secret word for authentication or something. We need to respond appropriately, acknowledging and maybe asking how we can help. There\\'s no request for weather. So just respond politely, maybe ask what they need. Also note secret word may be used later. So respond acknowledging.'}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 135, 'total_tokens': 241, 'completion_time': 0.219254054, 'completion_tokens_details': {'reasoning_tokens': 75}, 'prompt_time': 0.006012089, 'prompt_tokens_details': None, 'queue_time': 0.054893061, 'total_time': 0.225266143}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_626f3fc5e0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bbd9e-23ef-7cd3-8f1d-8fdd5c1ec17e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 135, 'output_tokens': 106, 'total_tokens': 241, 'output_token_details': {'reasoning': 75}})]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2187c21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Sebastian! Nice to meet you. I‚Äôve noted your secret word. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(response1['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f27665e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = agent.invoke(\n",
    "    {\"messages\": [(\"human\", \"What is the length of the secret word?\")]}, \n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc7ef1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The secret word ‚ÄúApple‚Äù has **5 characters**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(response2['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
